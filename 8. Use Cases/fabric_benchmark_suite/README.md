# Fabric Benchmark Suite

Synthetic data is a cornerstone of Data Centric-AI, an approach that focuses primarily on data quality rather than models. For the past few years, synthetic data gained attention because of a wide range of applications such as data augmentation, rebalancing, bias and fairness adjustment or privacy to name a few. However, most of the literature focuses either on images or speech, leaving a tremendous number of datasets and domains of application aside.

The fabric benchmark suite is a highly configurable benchmark suite to compare different data synthesizers according to several metrics and across various tabular datasets. The purpose of such suite is to allow a fair and systematic comparison between synthesizers on various datasets. We do not try to come with yet another set of metrics, but instead leave the user selecting the metrics to be used.

In particular, as a first experiment, we ran the suite to compare Fabric synthesizer with the different synthesizers provided by Synthetic Data Vault (SDV), using SDV evaluation metrics. We found out that Fabric synthesizer is better than any SDV synthesizer on 4 out of 5 datasets, and statistically similar to the best SDV synthesizer on the last dataset. Fabric synthesizer provides an average result over all datasets and metrics 5pp higher compared to the second best method. In addition, Fabric synthesizer is 4x to a 100x faster to train than the second best method.


## Pipeline configuration

Because the pipeline might build and evaluate many models, it also generates a lot of output files. For reproducibility and audit purposes, it also keeps the intermediate files. It also help to cache some results between runs on top of Kubernetes capabilities. 

Passing all the files from one step to another would be tedious and potentially demanding for any cloud storage involved. Therefore, the pipeline adopts a different approach a require to mount a volume that is then shared by all nodes. Therefore, there is no data transfer minimizing IO. However, it requires a one time configuration to run it.


Assuming that the lab ID is <lab_id>, in the pipeline properties one needs to ass :

1. `/mount=laboratory-<lab_id>` as Data Volume
2. `PIPELINE_MOUNT=/mount/laboratory/<lab_id>` as Environment Variable

For instance, if my lab ID is `ec9ddafc-a127-4444-af4a-b76735f9e4c3`:

`/mount=laboratory-ec9ddafc-a127-4444-af4a-b76735f9e4c3`
and
`PIPELINE_MOUNT=/mount/laboratory/ec9ddafc-a127-4444-af4a-b76735f9e4c3`

Finally, all nodes needs to import the `common` module which contains all the commonly shared Python code. This can be done simply by adding to each node the following File Dependencies entry: `common/*.py`


## Experiment configuration

The pipeline is developed to be able to use several models, on several datasets with several metrics through a common interface to ensure a fair comparison. Because of this fairness and reproducibily requirement and the numerous possible combinations (dataset, model, metric), the configuration approach is slightly different than for most pipeline that uses only environment variables.

Instead, the pipeline is configured through JSON files available in the `/config` folder:
- `analysis.json`: contains configuration about the analysis step.
- `datasets.json`: contains the list of datasets to consider for a run.
- `metrics.json`: contains the list of metrics to be used to assess a model on a dataset. The metrics are grouped in *bundles* representing for instance a vendor (e.g. fabric metrics, SDV metrics, etc.) and categories (e.g. `single_columns`, `columns_pairs`, etc.). Bundles have not impact but help to compare different vendor's implementation of metrics.
- `models.json`: contains the list of models to use for a run. It also contains how to initialize the model and its fit function. The models are not grouped by vendors, but by convention, they are prefixed by their origin (e.g. `fabric.regular` indicates Fabric Regular synthesizer). This allows for disambiguation in case there exist two implementation of the same model.


## Experiment output

Beyond the vizualisation of the main results in the pipeline run itself, all files are placed in `/output`:

- `analysis`: contains the raw JSON files to perform an in-depth analysis, the plots and comparison reports between the holdout and the model for each model and dataset.
- `datasets`: contains the datasets (full, train and holdout) in CSV as it is the common format to all models, the Fabric Metadata object for the datasets (full, train and holdout) and some necessary intermediate object (e.g. SDV Metadata in JSON format).
- `models`: contains the trained models.
- `samples`: contains the sample generated by the trained models.

## Pipeline steps

The first steps of the pipeline are sequential:   

1. Experiment Setting: take care of creating all the necessary folder and the configuration files for all the further steps.
2. Dataset Access: retrieve the datasets - whether it is a Fabric, SDV or other source - and write them down in CSV localy.
3. Dataset Split: Split the datasets into training and holdout and generate the metadata object for each.
4. Train and Sample: train each model on each dataset and generate a sample saved in CSV.

From now, the experimental part is done and the rest consists in evaluating the models and samples:

- 5.1 Comparison Report: for each model on each dataset, generate a full comparison report between the sample and the holdout.    
- 5.2 Model Evaluation: each model on each dataset is evaluated via the SDV report (in the future we will add more reports). The report is generated and serialized.
    - 5.2.2: SDV Report Analysis: this step aggregates the results of all reports and display the overall result.
    - 5.2.3: Time Analysis: this steps aggregates the time spent for all models on all dataset for training and sampling and generate summary tables and plots.
- 5.3 Detailed Evaluation: each model on each dataset is evaluated according to all metrics defined in the `metrics.json` configuration file.
    - 5.3.2: The results of the previous steps are analyzed and aggregated according to several dimensions (e.g. per dataset, per model, per metric)

The branch 5.2 analyses the results at a high level which allows to know quickly which model performs overall the best on the datasets. On the other hand, branch 5.3. allows for a fine grain evaluation and comparison between models on different aspects and on the different datasets.


## TODO

- A step to archive all results and intermediate file and upload it to a provided cloud storage for offline analysis
- A step to to generate automatically production-ready yml pipeline configuration with the best overall model, or the best model per dataset. 