{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e1986d-3cf2-423b-b7f1-a48a89da047b",
   "metadata": {},
   "source": [
    "# Sequential data synthesis - Time dependent financial transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278736b-5b08-4deb-b1e5-13b4d7daf0b8",
   "metadata": {},
   "source": [
    "### Import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c29845e2-45d9-4c32-9172-2a13b2b72c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "# Imports the packages that are needed\n",
    "#YData package specific packages\n",
    "import os\n",
    "\n",
    "from ydata.connectors import GCSConnector, LocalConnector\n",
    "from ydata.connectors.filetype import FileType\n",
    "from ydata.utils.formats import read_json\n",
    "\n",
    "from ydata.metadata import Metadata\n",
    "from ydata.utils.data_types import DataType\n",
    "\n",
    "import json\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    os.mkdir('outputs')\n",
    "except FileExistsError as e:\n",
    "    print('Directory already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "208ab6cb-bf38-4bea-83ed-89e602548b6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DATASET_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_138/2385573394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATASET_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATASET_PATH'"
     ]
    }
   ],
   "source": [
    "dataset_path = os.environ['DATASET_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31e1cd-3200-4df1-85cc-e48665d6614e",
   "metadata": {},
   "source": [
    "## Reading the data from the source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace7d23-79e6-426b-8b9e-1064fb8965ff",
   "metadata": {},
   "source": [
    "#### Using the connectors - Google Cloud Storage & Local filesystem\n",
    "\n",
    "To read the data from a given datasource (cloud storage, filesystem, etc.) it is possible to be done while using YData's scalable connectors. This connectors enable to read and write data from multiple different sources, but are only usable in the context of the lab where they where created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e132737c-8e64-4b43-a859-b2f3783aee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/distributed/client.py:1128: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+-------------+-----------+-----------+---------+\n",
      "| Package     | client    | scheduler | workers |\n",
      "+-------------+-----------+-----------+---------+\n",
      "| cloudpickle | 2.0.0     | 1.6.0     | None    |\n",
      "| distributed | 2021.10.0 | 2022.01.1 | None    |\n",
      "| msgpack     | 1.0.2     | 1.0.3     | None    |\n",
      "+-------------+-----------+-----------+---------+\n",
      "Notes: \n",
      "-  msgpack: Variation is ok, as long as everything is above 0.6\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2022-02-13 22:22:33,193 [CONNECTOR] - Init data types inference.\n",
      "INFO: 2022-02-13 22:22:41,011 [CONNECTOR] - Data types infered.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the YData's connector\n",
    "token = read_json('credentials/gcs_credentials.json')\n",
    "read_connector = GCSConnector('ydatasynthetic', keyfile_dict=token)\n",
    "\n",
    "# Read the data from the Cloud Storage \n",
    "data = read_connector.read_file(dataset_path, file_type = FileType.CSV)\n",
    "\n",
    "#Filter the data based on the columns required for the use case\n",
    "data = data.select_columns(columns=['account_id', 'date', 'type', 'amount', 'k_symbol', 'balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0231e956-052c-490f-a904-7ca1b087566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columns': {'account_id': 'int', 'date': 'int', 'type': 'string', 'amount': 'float', 'balance': 'float', 'k_symbol': 'string'}, 'target': None, 'shape': ((792546, 263774), 6), 'dataset_attr': None, 'warnings': {'skewness': ['amount', 'balance'], 'missing': ['k_symbol']}}\n"
     ]
    }
   ],
   "source": [
    "## Metadata, inicial calculation\n",
    "metadata = Metadata()\n",
    "metadata(data)\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9ce17b-98d7-4c25-9d4d-2c3b403d2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init metadata with time-series attributes\n",
    "dataset_attrs = {\n",
    "    \"sortbykey\": \"date\",\n",
    "    \"entity_id_cols\": [\"account_id\"],\n",
    "    \"autoregressive_cols\": [\"k_symbol\", \"balance\"]\n",
    "}\n",
    "\n",
    "metadata_ts = Metadata()\n",
    "metadata_ts(data, dataset_attrs=dataset_attrs)\n",
    "\n",
    "#Updating the data type from a column\n",
    "metadata_ts.columns['date'].datatype = DataType.NUMERICAL\n",
    "\n",
    "data_columns = metadata_ts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43d55a3d-07a0-4e79-b503-bdff62bd0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columns': {'account_id': 'int', 'date': 'int', 'type': 'string', 'amount': 'float', 'balance': 'float', 'k_symbol': 'string'}, 'target': None, 'shape': ((792546, 263774), 6), 'dataset_attr': DatasetDetails(sortbykey='date', entity_id_cols=['account_id'], conditioning_col=None, entity_attrs=None, autoregressive_cols=['k_symbol', 'balance']), 'warnings': {'skewness': ['amount', 'balance'], 'missing': ['k_symbol']}}\n"
     ]
    }
   ],
   "source": [
    "print(metadata_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939f878-98c7-49a7-a7f2-987fdd7f8acc",
   "metadata": {},
   "source": [
    "## Preparing the outputs for the next pipeline step\n",
    "To ensure that the required elements are shared from one step to the other of the pipeline we have to output the elements that we will need downstream the pipeline. In this particular example, we want to ensure both the dataset and the calculated metadata to avoid duplicated calculations and queries to external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deb73305-560c-48f3-95a0-168886a6e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the output of the dataset\n",
    "output_data = data.to_pandas()\n",
    "output_data.to_csv('outputs/real_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d892c-7932-404c-a9bf-edc10753f52e",
   "metadata": {},
   "source": [
    "Note that for bigger datasets it is recommended to write the intermediate steps into a remote storage (ObjectStorage, FileStorage, RDBMS, etc.)\n",
    "The platform have a limited amount of space in terms of storage, not to mention traceability and monitoring of the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae29b9a-865a-4fe3-be4d-e5f4bb187453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('outputs/dataset_attrs.json', 'w') as f_metadata:\n",
    "    json.dump(dataset_attrs, f_metadata)\n",
    "    \n",
    "with open('outputs/shape.json', 'w') as shape:\n",
    "    json.dump(data.shape(lazy_eval=False), shape)\n",
    "    \n",
    "with open('outputs/data_columns.pkl', 'wb') as f_columns:\n",
    "    pkl.dump(data_columns, f_columns, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81e94a76-7f08-42d8-8b09-3c182749e9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account_id', 'date', 'type', 'amount', 'balance', 'k_symbol']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(metadata_ts.columns.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24d5a3d7-c058-4d04-99f5-feec8d5a4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the visualization of the table.\n",
    "\n",
    "metadata = {\n",
    "    'outputs' : [{\n",
    "      'type': 'table',\n",
    "      'storage': 'inline',\n",
    "      'format': 'csv',\n",
    "      'header': list(metadata_ts.columns.keys()),\n",
    "      'source': 'outputs/real_data.csv'\n",
    "    }]\n",
    "  }\n",
    "\n",
    "with open(\"mlpipeline-ui-metadata.json\", 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
