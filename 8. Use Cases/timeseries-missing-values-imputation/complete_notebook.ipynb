{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d6a8be-0967-4685-b94d-fa456dece783",
   "metadata": {},
   "source": [
    "# Imputation of Missing Values for Wind Stations\n",
    "\n",
    "When taking measurements with devices, their malfunctioning and failure need to be considered. \n",
    "The collected measurements often end up being incomplete and with missing values (actual NaN). \n",
    "In order to extract the full information from these devices, YData's Missing Imputer is coming to rescue. \n",
    "\n",
    "For this use case we are going to work with wind measurements.\n",
    "The data consists in a time series composed by multiple wind wands. The wands are 10 in total, and all of them have a certain amount of data that is missing. \n",
    "Also, 2 devices have no records at all. (Cold Start Meters). <br>\n",
    "With the help of YData's Algorithms we are going to impute the missing values within the time series and upsample their granularity in order to have more information on what happens between the hours. We will also be able to generate months into the future that have not been observed yet and recreate from zero cold start meters.\n",
    "<br> <br>\n",
    "<div align=\"center\">\n",
    "    <img align='center' width=\"650\" height=\"300\" src=\"diagram.png\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c7714-f80f-432d-a179-629323a8e2a9",
   "metadata": {},
   "source": [
    "## 0 - Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd78912-3093-4d20-9092-e12f777002a2",
   "metadata": {},
   "source": [
    "### 0.1 - Imports\n",
    "\n",
    "Load the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24df2867-93f7-4eee-a3b1-a1becb2d9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install ipywidgets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9b439d-3e98-4383-a82b-5249f1e561b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from typing import List\n",
    "from pandas import concat\n",
    "from ipywidgets import interact\n",
    "\n",
    "# YDATA Proprietary Lib\n",
    "from ydata.dataset import Dataset\n",
    "from ydata.connectors import GCSConnector\n",
    "from ydata.utils.formats import read_json\n",
    "from ydata.quality.impute.timeseries import TSMissingImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3063e5b-60f4-4c35-ad24-4c9ceaf79330",
   "metadata": {},
   "source": [
    "### 0.2 - Auxiliary Functions\n",
    "\n",
    "The auxiliary functions are custom-designed utilities developed for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecec8766-bc24-4ef9-ae1b-4bf244eebb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factors import save_json\n",
    "from utils import setting_index_data\n",
    "from results import meter_statistics\n",
    "from upsample import upsample_stations\n",
    "from preprocess import preprocess_data\n",
    "from imputation import get_cold_start_meters, get_proxy_data, resample_station_data, data_boundaries, load_factors, resample\n",
    "from results_viz import plot_timeseries, compare_acf, plot_multicolor_line, plot_hist, compare_lags, plot_metric_byperiod, polar_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a72ca-06f6-42ae-a665-a4319caf577a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - Load Data\n",
    "Time Span of Data: 26 November 2018 - 1 March 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85de2c15-f389-44d5-acc0-bced362dfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the credentials\n",
    "credentials = read_json('gcs_credentials.json')\n",
    "\n",
    "# Create the connector for Google Cloud Storage\n",
    "connector = GCSConnector('ydatasynthetic', gcs_credentials=credentials)\n",
    "\n",
    "# Read the first extraction data 26 Nov 2020 to 15 Jul 2021\n",
    "data = connector.read_file('gs://pipelines_artifacts/wind_measurements_pipeline/data/ethiopia_wind_lot_nan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b28d17-a336-4a6b-8d51-4d999e9b750c",
   "metadata": {},
   "source": [
    "## 2 - Data Processing\n",
    "\n",
    "_Train_: 26 November 2018 - 1 March 2021\n",
    "\n",
    "_Holdout_: from 2 March 2021 onwards \n",
    "\n",
    "Steps:\n",
    "  1. Select the relevant columns\n",
    "  2. Convert to Pandas DataFrame\n",
    "  3. Cast to the correct data types\n",
    "  4. Join the readings from both extractions into a single table\n",
    "  5. Split into train and holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac128130-33cd-479c-84e9-3dd4bc54f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, holdout = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2163e4-dc40-44fd-a1a3-f81b013adcc7",
   "metadata": {},
   "source": [
    "### 2.1 - Data Before Imputation\n",
    "\n",
    "Visualize the data over time, before the imputation of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8718966e-4aa0-4309-8a3e-b81bc0948733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12851b8d3454465bdf82a0f47291473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='meter', options=('tuluguled1', 'seladingay', 'mega', 'sure', 'kebrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(meter=set(train['station']))\n",
    "def plot_meter_data(meter):\n",
    "    meter_original = train[train['station']==meter][:8000]\n",
    "    print(' \\n Percentage of missing data: {percentage:.2f}%'.format(percentage=meter_original.speed.isna().sum()/len(meter_original)*100))\n",
    "    plot_timeseries(data=meter_original, feature='speed', title='Original Data for '+ meter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b40454-fd8c-4d86-91be-06a324738105",
   "metadata": {},
   "source": [
    "As you can see, the data is all fractioned and full of gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e71bc-4e3c-4e8c-adaf-c27d1d067c31",
   "metadata": {},
   "source": [
    "## 3 - Calculate Factors \n",
    "\n",
    "In order to have more information about the behaviour of the data we calculate the average windspeed per month for meters in relevant provinces. These values will help the algorithm to better model the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5144ef5-9313-4fe2-bf62-3e7d00278f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [2018, 2019, 2020, 2021]\n",
    "# Internal IDs that identify stations which are in the same provinces\n",
    "meter_ids = read_json('meter_ids.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef52b2e-bbe6-4795-a7f6-a9f0699bbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_avg_per_year(connector: GCSConnector, meter_ids: List, years=YEARS):\n",
    "    \"Calculates the monthly factor over the anual average, per year, for meters within same provinces as the original data.\"\n",
    "\n",
    "    # create a map of each month to corresponding index\n",
    "    meses = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
    "             'august', 'september', 'october', 'november', 'december']\n",
    "\n",
    "    months = {k : v for (k, v) in zip(meses, range(1, len(meses) + 1))}\n",
    "\n",
    "    factors = []  # will contain a Series of monthly averages over anual, per each year\n",
    "\n",
    "    for year in years:\n",
    "        filepath = f'gs://pipelines_artifacts/wind_measurements_pipeline/data/df_for_factors_{str(year)}.csv'\n",
    "        df = connector.read_file(filepath, assume_missing=True).to_pandas().set_index('name_station')  # read the yearly monthly averages\n",
    "        df = df[df.index.isin(meter_ids)]                                                # filter for meters in same provinces\n",
    "        df = df.dropna()                                                                 # drop the missing values\n",
    "        for month in months.keys():                                                      # calculate factor as ratio of monthly average\n",
    "            df[month] = df[month] / df['yearly']                                          # over the anual average\n",
    "        factors.append(df[months.keys()].mean().copy())\n",
    "\n",
    "    # Aggregate data\n",
    "    agg_data = concat(factors, axis=1)\n",
    "    agg_data.columns = years\n",
    "    agg_data = agg_data.rename(index=months)\n",
    "    agg_data['avg'] = agg_data.mean(axis=1)\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c38200e-80c8-4b64-b971-b20034b8951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = get_monthly_avg_per_year(connector=connector, meter_ids=meter_ids)\n",
    "factors = load_factors({'windspeed': factors['avg'].to_dict()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13334a76-5bb3-4d4a-a26d-12fc8ba89493",
   "metadata": {},
   "source": [
    "## 4 - Imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480fc83-e6ac-42e1-b61d-d2247ef281d3",
   "metadata": {},
   "source": [
    "### 4.1 - Cold Start Meters\n",
    "\n",
    "Training on cold-start meters (i.e. without any observed values) should be made in separate from the rest of the meters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dbca6-f539-40f1-b95d-7020abba1b69",
   "metadata": {},
   "source": [
    "#### 4.1.1 - Data Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d206e105-e963-43d7-a5da-416fe1d8acb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aysha1', 'gode1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of cold-start meters\n",
    "cold_start_meters = get_cold_start_meters(train)\n",
    "cold_start_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fd11698-18e1-4596-a838-4cbd49f47449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stations that can serve as proxy data for the cold-start meters\n",
    "proxy_stations = {\n",
    "    'aysha1': ['diredawa1', 'tuluguled1'],\n",
    "    'gode1': ['seladingay', 'ziway'],\n",
    "}\n",
    "proxy_data = get_proxy_data(proxy_stations, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92aa070f-aa36-4b65-8316-4816bc3c394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data for cold-start meters only.\n",
    "cold_start_data = train[train['station'].isin(cold_start_meters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bcb5f1-1cde-4d62-a6e8-f9f384ffd1dd",
   "metadata": {},
   "source": [
    "#### 4.1.2 - Boundaries \n",
    "\n",
    "Apply the data boundaries to each dataframe used as proxy data for cold start in order to avoid out of bound values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b783b7-d641-4766-83f4-90b6d2971572",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_data = {k: data_boundaries(v, replace_na=True) for (k,v) in proxy_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf5aa8-9a43-4fcb-910b-e3b3d83d1cea",
   "metadata": {},
   "source": [
    "#### 4.1.3 - Imputer \n",
    "\n",
    "The TSMissingImputer is responsible to impute the missing values for time-series.\n",
    "- Learns the temporal dynamics from the observed values\n",
    "- Supports multiple entities with the `partition_by` parameter\n",
    "- Follows the usual scikit-learn method interfaces (e.g. fit, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3005b2e-f272-47ac-a751-4667e470f75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSMissingImputer()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cold_imputer = TSMissingImputer()\n",
    "\n",
    "# Train the Imputer\n",
    "cold_imputer.fit(cold_start_data, partition_by='station', num_cols=['speed'], proxy_data=proxy_data, add_factors=factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e4ef4c-26e7-4fcf-b9ec-0457d710147f",
   "metadata": {},
   "source": [
    "#### 4.1.4 - Impute Full Year\n",
    "\n",
    "Construct a full year of data, on hourly basis, for devices with observed readings. For each hour, the average of windspeed/winddirection is calculated and used as ground-truth for observed readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d890fabc-47ab-498e-aa64-efd900d47f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of a whole year for all the meters with observed values.\n",
    "whole_year = resample_station_data(cold_start_data)\n",
    "\n",
    "# Apply the missing values imputation to reconstruct a whole year of data.\n",
    "cold_reconstructed = cold_imputer.transform(whole_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28752cdf-a3c0-49c1-aaae-bc3e9f823728",
   "metadata": {},
   "source": [
    "#### 4.1.5 - Impute Holdout \n",
    "\n",
    "Construct a full month of holdout, on hourly basis, for devices with observed readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362bbbdd-52f0-448f-9b33-6a5ea7ba161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for cold-start meters on holdout data.\n",
    "cold_holdout = holdout[holdout['station'].isin(cold_start_meters)]\n",
    "whole_holdout = resample_station_data(cold_holdout, start_ts='2021-09-01', end_ts='2021-10-01')\n",
    "cold_holdout_reconstructed = cold_imputer.transform(whole_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4cdd7d-04e5-48fa-9771-a50855de97e1",
   "metadata": {},
   "source": [
    "#### 4.1.6 - Data Validation\n",
    "\n",
    "After the reconstruction check that no value is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c775d740-2198-4020-b70b-f8c89e00ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cold_reconstructed.isna().sum().sum() == 0, \"The reconstructed dataset contains missing values after reconstruction.\"\n",
    "assert cold_holdout_reconstructed.isna().sum().sum() == 0, \"The reconstructed dataset of holdout contains missing values after reconstruction.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177df281-f35a-4f54-b614-12da545cbeb5",
   "metadata": {},
   "source": [
    "#### 4.1.7 - Post Processing \n",
    "\n",
    "The imputation of time-series is applicable to any type of numerical data and thus agnostic to energy-specific boundaries of wind measurements. To guarantee adequacy for wind speed and direction, we enforce that wind speed cannot be negative and that wind direction should range within degree angles (between 0 and 360)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88d3d9bb-f865-4b9b-b3c6-c5efe4277366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocess the training data\n",
    "postprocessed = data_boundaries(data=cold_reconstructed)\n",
    "\n",
    "# Postprocess the holdout data\n",
    "postprocessed_holdout = data_boundaries(data=cold_holdout_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f736379f-dd71-4382-bca0-d963f217508f",
   "metadata": {},
   "source": [
    "### 4.2 - Standard Meters \n",
    "\n",
    "Meters with observed values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bcbb30e-630f-46f9-9292-5cc1280b7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the cold-start meters from the train data.\n",
    "train_data = train[~train['station'].isin(cold_start_meters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546b337-e3e3-4d9f-baed-89bc3f07b601",
   "metadata": {},
   "source": [
    "#### 4.2.1 - Data Boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82ede782-349f-4a57-9b23-0968954a5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_boundaries(train_data, replace_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386620d3-5d5b-48ad-9142-c30bf1ad62b8",
   "metadata": {},
   "source": [
    "#### 4.2.2 - Imputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33058d50-a5d1-45af-9307-ad18e07107b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSMissingImputer()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Imputer\n",
    "imputer = TSMissingImputer()\n",
    "# Train the Imputer\n",
    "imputer.fit(train_data, partition_by='station', num_cols=['speed'], add_factors=factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0caf8ca-df65-40df-8dac-58373a2c0ffc",
   "metadata": {},
   "source": [
    "#### 4.2.3 - Impute Full Year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bdbe1da-2043-4a94-9453-0e40cf2c940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of a whole year for all the meters with observed values.\n",
    "whole_year = resample_station_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8e91f6d-d893-449c-884f-5daec938cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the missing values imputation to reconstruct a whole year of data.\n",
    "reconstructed = imputer.transform(whole_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75a037-cc31-4cc1-95b8-d30aa8cefb15",
   "metadata": {},
   "source": [
    "#### 4.2.4 - Impute Holdout  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "047303fa-5730-4cd2-bb70-e0add9a361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the cold-start meters from holdout data.\n",
    "standard_holdout = holdout[~holdout['station'].isin(cold_start_meters)]\n",
    "whole_holdout = resample_station_data(standard_holdout, start_ts='2021-03-01', end_ts='2021-04-30')\n",
    "holdout_reconstructed = imputer.transform(whole_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d7fdf-5eef-42bf-83f4-a422ea9806ca",
   "metadata": {},
   "source": [
    "#### 4.2.5 - Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c267434-6f02-4b6a-9ee4-71bc5f5c56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After reconstruction, no value should be missing\n",
    "assert reconstructed.isna().sum().sum() == 0, \"The reconstructed dataset contains missing values after reconstruction.\"\n",
    "assert holdout_reconstructed.isna().sum().sum() == 0, \"The reconstructed dataset of holdout contains missing values after reconstruction.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead5e0f-586d-4c17-a225-b157919db071",
   "metadata": {},
   "source": [
    "#### 4.2.6 - Post Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fb1b58a-108c-4621-b557-8edb764aed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocess the training data\n",
    "postprocessed = data_boundaries(data=reconstructed)\n",
    "\n",
    "# Postprocess the holdout data\n",
    "postprocessed_holdout = data_boundaries(data=holdout_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94709a88-da01-4d4c-b79b-01f593a93eb1",
   "metadata": {},
   "source": [
    "## 5 - Results Visualizations \n",
    "\n",
    "Comparison between original and generated data through visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2092566f-408d-453c-979a-083d7e9e1f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729a5059448442e59956c0acb8f71a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='meter', options=('tuluguled1', 'mega', 'seladingay', 'diredawa1', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(meter=set(reconstructed['station']))\n",
    "def plot_meter_results(meter):\n",
    "    meter_reconstructed = reconstructed[reconstructed['station']==meter][:8000]\n",
    "    meter_original = train[train['station']==meter]\n",
    "    #[:8000]\n",
    "\n",
    "    # Convert the original data into resampled for full year \n",
    "    original_year = resample_station_data(meter_original)\n",
    "\n",
    "    meter_statistics(original_year)\n",
    "    plot_timeseries(data=original_year, feature='speed', title='Original Data for '+ meter)\n",
    "    plot_multicolor_line(original_year, meter_reconstructed, title='Reconstruction of '+ meter)\n",
    "    compare_acf(original_year, meter_reconstructed, title='Hourly Autocorrelation')\n",
    "    plot_hist(original_year, meter_reconstructed, title='Distribution Overlap - Original and Reconstructed Series - '+ meter)\n",
    "    compare_lags(original_year, meter_reconstructed, 'speed')\n",
    "    plot_metric_byperiod(original_year, meter_reconstructed, 'speed')\n",
    "    polar_graph(original_year, meter_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293dfdb-5ffa-4c12-bbbf-5babfd6a3334",
   "metadata": {},
   "source": [
    "## 6 - Upsampling \n",
    "\n",
    "Upsample the measurements to 15-minutes frequency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3727e81e-f7f4-4a46-8b92-6825e5d86a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reconstructed data for the given device\n",
    "reconstructed_upsampled = upsample_stations(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3830cd54-1e6e-46e3-b780-75589ecfc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the original data for 15 minute frequency\n",
    "original_upsampled = resample(train, ts_col='timestamp', ts_freq='15T', partition_by='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e6ae9-a4f8-4699-9b50-89acdeaa85b9",
   "metadata": {},
   "source": [
    "## 7 - Store Data\n",
    "\n",
    "Load data into the cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f03b9246-8697-456b-b727-ca914e33b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the credentials\n",
    "credentials = read_json('gcs_credentials.json')\n",
    "\n",
    "# Create the connector for Google Cloud Storage\n",
    "connector = GCSConnector('ydatasynthetic', gcs_credentials=credentials)\n",
    "\n",
    "# Store the reconstructed data upsampled to 15 minutes\n",
    "connector.write_file(reconstructed_upsampled, 'gs://pipelines_artifacts/wind_measurements_pipeline/outputs/reconstructed_upsampled.csv', index=True)\n",
    "\n",
    "# Store the original data upsampled to 15 minutes\n",
    "connector.write_file(original_upsampled, 'gs://pipelines_artifacts/wind_measurements_pipeline/outputs/original_upsampled.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
