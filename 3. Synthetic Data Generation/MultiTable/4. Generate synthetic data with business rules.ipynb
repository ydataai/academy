{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869be392",
   "metadata": {},
   "source": [
    "# Multi-Table Synthesis with Business Rules\n",
    "\n",
    "Combining synthetic data with traditional anonymization enhances privacy and data utility while ensuring compliance with regulations. Synthetic data reduces re-identification risks by not being directly tied to individuals, preserving the usefulness of data for analysis. This approach also facilitates safer data sharing and collaboration by adding an extra layer of privacy protection that allows to replicate the same schema while protecting certain identifiers, like zip-codes or even unique identifiers, making it a strategic choice for organizations handling sensitive information.\n",
    "\n",
    "In this notebook we will be exploring how to combine the benefits of the `MultiTableSynthesizer`with YData Fabric Anonymizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e33ee-c9cb-407e-827e-d55ae6a47911",
   "metadata": {},
   "source": [
    "## Getting your database from the Data Catalog\n",
    "\n",
    "In this example we have create our database in a MySQL server and [created a Dataset in Fabric Data Catalog](https://docs.sdk.ydata.ai/0.10/get-started/create_multitable_dataset/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99519bbc-4a71-4808-8b6d-57cb46119159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing YData's packages\n",
    "from ydata.labs import DataSources\n",
    "\n",
    "# Reading the Dataset from the DataSource\n",
    "datasource = DataSources.get(uid='{insert-datasource-uid}')\n",
    "\n",
    "dataset = datasource.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877abd3b",
   "metadata": {},
   "source": [
    "## Training & sampling a Database Synthetic Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdec661",
   "metadata": {},
   "source": [
    "The calculated features functionality allows the generation of specific columns based on data from other columns according to the business rules specified in custom functions.\n",
    "\n",
    "In this example, the `Berka` database transactions table can be considered a time series. For that reason, the table **trans** will to be set as a `timeseries` and the column `date` as the table time order reference (**sortbykey**). For that reason we need to calculate a new `MultiMetadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308c304c-6c68-4969-9f6a-e0932dfb3bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydata/.venv/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 9.55 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ydata/.venv/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 9.65 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ydata/.venv/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 9.56 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ydata/.venv/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 9.55 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ydata.metadata.multimetadata import MultiMetadata\n",
    "\n",
    "dataset_type = {\n",
    "    'trans': 'timeseries'\n",
    "}\n",
    "\n",
    "dataset_attrs = {\n",
    "    'trans': {\n",
    "        'sortbykey': 'date',\n",
    "        'entities': []\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata = MultiMetadata(dataset, dataset_attrs=dataset_attrs, dataset_type=dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851c37b-6f50-4a54-bee9-114f4a115690",
   "metadata": {},
   "source": [
    "In this example, the following columns are calculated features:\n",
    "- The `full_name` column from the `client` table is generated by concatenating the first and last names of each client, which are available in the `first_name` and `last_name` columns of the same table.\n",
    "- The `a10_sum` column from the `client` table is generated by summing all the values from the `a10` column of the `district` table for each client. Since this is an inter-table calculated feature (i.e., several tables are used), there is a need to establish the relationship between the tables (in this case, between the `client` and the `district`). The user should include the primary and foreign keys in the base columns, and establish the relationship inside the custom function (see the `get_a10_sum` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc6af6d-6d9c-4abf-8c2e-e7531327e84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_full_name(first_name, last_name):\n",
    "    full_names = []\n",
    "    for ix in range(first_name.shape[0]):\n",
    "        full_names.append(first_name[ix].strip() + \" \" + last_name[ix].strip())\n",
    "    return np.asarray(full_names)\n",
    "\n",
    "def get_a10_sum(client_id, district_id, a1, a10):\n",
    "    a1_s = pd.Series(a1, name=\"a1\")\n",
    "    a10_s = pd.Series(a10, name=\"a10\")\n",
    "    district_data = pd.concat([a1_s, a10_s], axis=1)\n",
    "    a10_sum = pd.Series(0, index=client_id)\n",
    "    for c, d in zip(client_id, district_id):\n",
    "        a10_sum[c] = district_data[district_data[\"a1\"] == d][\"a10\"].sum()\n",
    "    return a10_sum.values\n",
    "\n",
    "calculated_features=[\n",
    "    {\n",
    "      \"calculated_features\": \"client.full_name\",\n",
    "      \"function\": get_full_name,\n",
    "      \"calculated_from\": [\"client.first_name\", \"client.last_name\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74693170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-19 12:09:27,858 (1/9) - Fitting table: [district]\n",
      "INFO: 2024-11-19 12:09:34,692 [SYNTHESIZER] - Number columns considered for synth: 16\n",
      "INFO: 2024-11-19 12:09:35,407 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:09:35,410 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:09:35,420 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:09:35,421 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:09:35,792 (2/9) - Fitting table: [client]\n",
      "INFO: 2024-11-19 12:09:39,229 [SYNTHESIZER] - Number columns considered for synth: 23\n",
      "INFO: 2024-11-19 12:09:40,312 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:09:40,316 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:09:40,326 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:09:40,328 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:09:42,594 (3/9) - Fitting table: [account]\n",
      "INFO: 2024-11-19 12:09:45,310 [SYNTHESIZER] - Number columns considered for synth: 21\n",
      "INFO: 2024-11-19 12:09:46,259 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:09:46,262 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:09:46,269 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:09:46,271 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:09:47,255 [MULTITABLE] - Clearing tables from memory [{'district'}]\n",
      "INFO: 2024-11-19 12:09:47,256 (4/9) - Fitting table: [order]\n",
      "INFO: 2024-11-19 12:09:54,585 [SYNTHESIZER] - Number columns considered for synth: 11\n",
      "INFO: 2024-11-19 12:09:55,286 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:09:55,291 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:09:55,302 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:09:55,304 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:09:58,560 (5/9) - Fitting table: [trans]\n",
      "INFO: 2024-11-19 12:10:03,578 [SYNTHESIZER] - Number columns considered for synth: 15\n",
      "INFO: 2024-11-19 12:10:07,002 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:10:07,019 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:10:07,034 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:10:07,036 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:10:27,720 (6/9) - Fitting table: [disp]\n",
      "INFO: 2024-11-19 12:10:32,853 [SYNTHESIZER] - Number columns considered for synth: 6\n",
      "INFO: 2024-11-19 12:10:33,131 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:10:33,135 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:10:33,141 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:10:33,142 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:10:33,919 [MULTITABLE] - Clearing tables from memory [{'client'}]\n",
      "INFO: 2024-11-19 12:10:33,921 (7/9) - Fitting table: [card]\n",
      "INFO: 2024-11-19 12:10:34,985 [SYNTHESIZER] - Number columns considered for synth: 8\n",
      "INFO: 2024-11-19 12:10:35,304 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:10:35,309 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:10:35,315 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:10:35,317 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:10:35,723 (8/9) - Fitting table: [loan]\n",
      "INFO: 2024-11-19 12:10:37,382 [SYNTHESIZER] - Number columns considered for synth: 14\n",
      "INFO: 2024-11-19 12:10:38,058 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:10:38,061 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:10:38,067 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:10:38,069 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:10:38,866 [MULTITABLE] - Clearing tables from memory [{'account'}]\n",
      "INFO: 2024-11-19 12:10:38,867 (9/9) - Fitting table: [append]\n",
      "INFO: 2024-11-19 12:10:39,449 [SYNTHESIZER] - Number columns considered for synth: 3\n",
      "INFO: 2024-11-19 12:10:39,636 [SYNTHESIZER] - Starting the synthetic data modeling process over 1x1 blocks.\n",
      "INFO: 2024-11-19 12:10:39,639 [SYNTHESIZER] - Preprocess segment\n",
      "INFO: 2024-11-19 12:10:39,643 [SYNTHESIZER] - Synthesizer init.\n",
      "INFO: 2024-11-19 12:10:39,645 [SYNTHESIZER] - Processing the data prior fitting the synthesizer.\n",
      "INFO: 2024-11-19 12:10:39,723 [MULTITABLE] - Clearing tables from memory [{'append', 'disp', 'loan', 'trans', 'order'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ydata.synthesizers.multitable.model.MultiTableSynthesizer at 0x7fa5ec539ab0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ydata.synthesizers.multitable.model import MultiTableSynthesizer\n",
    "\n",
    "synth = MultiTableSynthesizer()\n",
    "synth.fit(dataset, metadata, calculated_features=calculated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a72b2c",
   "metadata": {},
   "source": [
    "To generate the synthetic data we call the `sample` method.\n",
    "\n",
    "Since there is a need to keep the consistency of the tables, as well as the referential integrity, to sample from trained synthesizers the number of records is set through a ratio based on the original number of records (e.g., 1.0 is equivalent to the size of the original database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c4399-911d-4b95-8786-149689d2ab74",
   "metadata": {},
   "source": [
    "### Generate data and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3727967-6a56-41f3-8799-c65ede1560f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MySQLConnector(\n",
      "  \n",
      "  uid='ac7ec4a8-ea81-4725-8c0d-b40b88db0c6a',\n",
      "  name='Berka database synth',\n",
      "  type=ConnectorType.MYSQL,\n",
      "  connection=Connection(host='data-science-mysql-41955.c1xxv3f18hni.eu-west-1.rds.amazonaws.com', port=3306),\n",
      "  database=berka_synth)\n"
     ]
    }
   ],
   "source": [
    "# Importing YData's packages\n",
    "from ydata.labs import Connectors\n",
    "# Getting a previously created Connector\n",
    "connector = Connectors.get(uid='{insert-connector-uid}')\n",
    "print(connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165781c6-058c-4e1a-8877-8e952bb586ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-19 12:59:13,588 (1/9) - Synthesizing table: district\n",
      "INFO: 2024-11-19 12:59:13,590 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:13,591 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:13,702 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:17,254 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:17,256 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:17,316 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:18,671 (2/9) - Synthesizing table: client\n",
      "INFO: 2024-11-19 12:59:19,009 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:19,011 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:19,528 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:19,629 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:19,631 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:19,765 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:20,100 (3/9) - Synthesizing table: account\n",
      "INFO: 2024-11-19 12:59:20,456 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:20,458 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:20,683 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:20,781 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:20,782 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:20,894 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:21,088 Persisting table [district]\n",
      "INFO: 2024-11-19 12:59:25,893 (4/9) - Synthesizing table: order\n",
      "INFO: 2024-11-19 12:59:25,993 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:25,995 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:26,580 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:26,662 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:26,665 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:26,764 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:27,028 (5/9) - Synthesizing table: trans\n",
      "INFO: 2024-11-19 12:59:27,199 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:27,202 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:30,776 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:31,179 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:31,181 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:31,950 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:32,616 (6/9) - Synthesizing table: disp\n",
      "INFO: 2024-11-19 12:59:32,618 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:32,620 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:32,953 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:33,018 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:33,019 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:33,107 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:33,238 Persisting table [client]\n",
      "INFO: 2024-11-19 12:59:38,525 (7/9) - Synthesizing table: card\n",
      "INFO: 2024-11-19 12:59:38,624 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:38,629 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:38,739 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:38,844 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:38,846 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:38,897 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:38,997 (8/9) - Synthesizing table: loan\n",
      "INFO: 2024-11-19 12:59:39,087 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:39,089 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:39,283 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:39,353 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:39,354 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:39,424 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:39,802 Persisting table [account]\n",
      "INFO: 2024-11-19 12:59:44,941 (9/9) - Synthesizing table: append\n",
      "INFO: 2024-11-19 12:59:44,943 [SYNTHESIZER] - Start generating model samples.\n",
      "INFO: 2024-11-19 12:59:44,945 [SYNTHESIZER] - Init Dask cluster for sampling.\n",
      "INFO: 2024-11-19 12:59:44,966 [SYNTHESIZER] - Postprocessing.\n",
      "INFO: 2024-11-19 12:59:44,995 let write into the connector None\n",
      "INFO: 2024-11-19 12:59:44,997 using kwargs {}\n",
      "INFO: 2024-11-19 12:59:45,031 [SYNTHESIZER] - Numerical clipping\n",
      "INFO: 2024-11-19 12:59:45,158 Persisting table [order]\n",
      "INFO: 2024-11-19 12:59:50,534 Persisting table [trans]\n",
      "INFO: 2024-11-19 13:00:11,718 Persisting table [disp]\n",
      "INFO: 2024-11-19 13:00:16,976 Persisting table [loan]\n",
      "INFO: 2024-11-19 13:00:21,941 Persisting table [append]\n",
      "INFO: 2024-11-19 13:00:26,461 Persisting table [card]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ydata.dataset.multidataset.MultiDataset at 0x7fa5ec984700>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth.sample(n_samples=1., connector=connector.connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0610a6cf-753b-489d-971f-5ffe559d5c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synth_data = connector.read_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41c859d8-4fa5-41ee-8e91-2c3c6fd1c682",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMultiDataset Summary \n",
      " \n",
      "\u001b[0m\u001b[1mNumber of tables: \u001b[0m9 \n",
      " \n",
      "  Table name  Num cols                         Num rows Primary keys Foreign keys Notes\n",
      "0    account         4  Number of rows not yet computed                                \n",
      "1     append         3  Number of rows not yet computed                                \n",
      "2       card         4  Number of rows not yet computed                                \n",
      "3     client         6  Number of rows not yet computed                                \n",
      "4       disp         4  Number of rows not yet computed                                \n",
      "5   district        16  Number of rows not yet computed                                \n",
      "6       loan         9  Number of rows not yet computed                                \n",
      "7      order         6  Number of rows not yet computed                                \n",
      "8      trans        10  Number of rows not yet computed                                \n"
     ]
    }
   ],
   "source": [
    "print(synth_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
