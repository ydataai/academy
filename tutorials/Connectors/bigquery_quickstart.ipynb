{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0eb9bfb6d2367f5ad13c071e9ca0a4847a6ac5b11a61adc2b39beabe858de7ad1",
   "display_name": "Python 3.7.9 64-bit ('ydata_tmp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Big Query Connector - Quick Start\n",
    "The BigQuery connector enables you to read/write data within BigQuery with ease and integrate it with YData's platform. \n",
    "Reading a dataset from BigQuery directly into a YData's `Dataset` allows its usage for Data Quality, Data Synthetisation and Preprocessing blocks.\n",
    "\n",
    "## Storage and Performance Notes\n",
    "BigQuery is not intended to hold large volumes of data as a pure data storage service. Its main advantages are based on the ability to execute SQL-like queries on existing tables which can efficiently aggregate data into new views. As such, for storage purposes we advise the use of Google Cloud Storage and provide the method `write_query_to_gcs`, available from the `BigQueryConnector`, that allows the user to export a given query to a Google Cloud Storage object."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata.connectors import BigQueryConnector\n",
    "from ydata.utils.formats import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your credentials from a file\\n\",\n",
    "token = read_json('../../.secrets/gcs_write_token.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Connector\n",
    "connector = BigQueryConnector(project_id='ydatasynthetic', keyfile_dict=token)"
   ]
  },
  {
   "source": [
    "%%time\n",
    "# Load a dataset\n",
    "data = connector.query(\n",
    "    \"SELECT * FROM dataset_test.cardio_data\"\n",
    ")\n",
    "data.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Load a sample of a dataset\n",
    "small_data = connector.query(\n",
    "    \"SELECT * FROM dataset_test.cardio_data\",\n",
    "    n_sample=10_000\n",
    ")\n",
    "small_data.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Check the available datasets\n",
    "connector.datasets"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the available tables for a given dataset\n",
    "connector.list_tables('dataset_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector.table_schema(dataset='dataset_test', table='cardio_data')"
   ]
  },
  {
   "source": [
    "## Advanced\n",
    "With `BigQueryConnector`, you can access useful properties and methods directly from the main class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the datasets of a given project\n",
    "connector.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the BigQuery Client\n",
    "connector.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset\n",
    "# connector.get_or_create_dataset(dataset='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a dataset. WARNING: POTENTIAL LOSS OF DATA\n",
    "# connector.delete_table_if_exists(dataset='', table='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a dataset. WARNING: POTENTIAL LOSS OF DATA \n",
    "# connector.delete_dataset_if_exists(dataset='')"
   ]
  },
  {
   "source": [
    "### Example #1 - Execute Pandas transformations and store to BigQuery"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to pandas\n",
    "# small_df = small_data.to_pandas()\n",
    "#\n",
    "# subset the columns\n",
    "# small_df = small_df[['age', 'gender', 'cardio']]\n",
    "#\n",
    "# Write results to BigQuery table\n",
    "# connector.write_table_from_data(data=small_df, dataset='connectors_dev', table='small_df')"
   ]
  },
  {
   "source": [
    "### Example #2 - Write a BigQuery results to Google Cloud Storage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query in BigQuery and store it in Google Cloud Storage\n",
    "# connector.write_query_to_gcs(query=\"SELECT age FROM dataset_test.cardio_data\", \n",
    "#                                 path=\"gs://ydata_development/connectors/bigquery_pipeline_age.csv\")"
   ]
  }
 ]
}