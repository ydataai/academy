{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c9bf11-14ea-47f7-9991-41cadba5dbe6",
   "metadata": {},
   "source": [
    "# Connectors - Databricks Delta Lake\n",
    "\n",
    "[YData SDK provides a seamless integration with Databricks Delta Lake](https://ydata.ai/), allowing you to connect,\n",
    "query, and manage your data in Delta Lake with ease. This section will guide you through the benefits,\n",
    "setup, and usage of the Databricks' connector within [ydata-sdk](https://pypi.org/project/ydata-sdk/).\n",
    "\n",
    "### Benefits of Integration\n",
    "Integrating ydata-sdk with Databricks offers several key benefits:\n",
    "\n",
    "- **Enhanced Data Accessibility:** Seamlessly access and integrate previously siloed data.\n",
    "- **Improved Data Quality:** Use YData's tools to enhance the quality of your data through data preparation and augmentation.\n",
    "- **Scalability:** Leverage Databricks' robust infrastructure to scale data processing and AI workloads.\n",
    "- **Streamlined Workflows:** Simplify data workflows with connectors and SDKs, reducing manual effort and potential errors.\n",
    "- **Comprehensive Support:** Benefit from extensive documentation and support for both platforms, ensuring smooth integration and operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358cb66-0916-4e24-8c3e-9ebbe89c1cbc",
   "metadata": {},
   "source": "### Authenticate with your account YData"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72403051-83e4-4586-8839-43ad4d1ea3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with your ydata-sdk token - https://dashboard.ydata.ai/\n",
    "import os\n",
    "\n",
    "os.environ['YDATA_LICENSE_KEY'] = '{add-your-key}'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create your Delta Lake connector",
   "id": "81db4afaa941118c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add the connection details - your databricks host, access token, access to an AWS S3 or Azure Delta lake, as well as the dataset details (catalog name, schema name, table and warehouse)\n",
    "HOST = 'insert-databricks-host'\n",
    "TOKEN = 'insert-token'\n",
    "\n",
    "AWS_ACCESS_KEY = 'insert-aws-key'\n",
    "SECRET_ACCESS_KEY = 'insert-secret'\n",
    "\n",
    "CATALOG = 'catalog-name'\n",
    "SCHEMA = 'schema-name'\n",
    "TABLE = 'table-name'\n",
    "WAREHOUSE = 'your-warehouse-name'"
   ],
   "id": "fe8bc968be09dd44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The Databricks Lakehouse requires several inputs:\n",
    "# Credentials to one of the following object storage (AWS S3 or Azure Blob Storage)\n",
    "# Databricks host and access token\n",
    "connector = DatabricksLakehouse(\n",
    "    host=HOST,\n",
    "    access_token=TOKEN,\n",
    "    staging_credentials={'access_key_id': AWS_ACCESS_KEY,\n",
    "                         'secret_access_key': SECRET_ACCESS_KEY},\n",
    "    catalog='catalog',\n",
    "    schema='schema',\n",
    "    cloud='aws'\n",
    ")"
   ],
   "id": "2059593b1fdd167f"
  },
  {
   "cell_type": "markdown",
   "id": "80f76f8b-1a8b-4606-a4c1-9ba90343daa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read from your Delta Lake\n",
    "\n",
    "Using the Delta Lake connector it is possible to:\n",
    "- Get the data from a Delta Lake table\n",
    "- Get a sample from a Delta Lake table\n",
    "- Get the data from a query to a Delta Lake instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275ccdbe-b220-4926-b481-5f871653c764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2024-06-11 22:01:11,445 Successfully opened session 01ef283e-1d15-1af3-98a1-f98419972ba9\n",
      "INFO: 2024-06-11 22:01:12,055 Successfully opened session 01ef283e-1d7e-1f4a-b6fb-a89fe5ff57a8\n",
      "\u001B[1mDataset \n",
      " \n",
      "\u001B[0m\u001B[1mShape: \u001B[0m(5000, 13)\n",
      "\u001B[1mSchema: \u001B[0m\n",
      "         Column Variable type\n",
      "0            id        string\n",
      "1           age           int\n",
      "2        gender           int\n",
      "3        height           int\n",
      "4        weight         float\n",
      "5         ap_hi           int\n",
      "6         ap_lo           int\n",
      "7   cholesterol           int\n",
      "8          gluc           int\n",
      "9         smoke           int\n",
      "10         alco           int\n",
      "11       active           int\n",
      "12       cardio           int\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define what warehouse you want to use for the activity\n",
    "#This method reads all the data records in the table\n",
    "table = connector.get_table(table='insert-table-name',\n",
    "                            warehouse='insert-warehouse-name')\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13e164e-666b-447d-a1e1-51f6c038b9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset \n",
      " \n",
      "\u001B[0m\u001B[1mShape: \u001B[0m(50, 13)\n",
      "\u001B[1mSchema: \u001B[0m\n",
      "         Column Variable type\n",
      "0            id        string\n",
      "1           age           int\n",
      "2        gender           int\n",
      "3        height           int\n",
      "4        weight         float\n",
      "5         ap_hi           int\n",
      "6         ap_lo           int\n",
      "7   cholesterol           int\n",
      "8          gluc           int\n",
      "9         smoke           int\n",
      "10         alco           int\n",
      "11       active           int\n",
      "12       cardio           int\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define what warehouse you want to use for the activity\n",
    "#This method reads a sample with number of records = provided sample size\n",
    "table_sample = connector.get_table_sample(table='insert-table-name',\n",
    "                                          warehouse='insert-warehouse-name',\n",
    "                                          sample_size=50)\n",
    "print(table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80412745-1df9-4bed-bb18-2cff91dabe91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2024-06-11 22:08:56,902 Successfully opened session 01ef283f-328f-115a-a977-2da27926d77a\n",
      "\u001B[1mDataset \n",
      " \n",
      "\u001B[0m\u001B[1mShape: \u001B[0m(5000, 13)\n",
      "\u001B[1mSchema: \u001B[0m\n",
      "         Column Variable type\n",
      "0            id        string\n",
      "1           age           int\n",
      "2        gender           int\n",
      "3        height           int\n",
      "4        weight         float\n",
      "5         ap_hi           int\n",
      "6         ap_lo           int\n",
      "7   cholesterol           int\n",
      "8          gluc           int\n",
      "9         smoke           int\n",
      "10         alco           int\n",
      "11       active           int\n",
      "12       cardio           int\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_output = connector.query(\"SELECT * FROM catalogname.schemaname.tablename;\",\n",
    "                               warehouse='insert-warehouse-name')\n",
    "print(query_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b263d-032c-473d-a10b-c51e0312e18a",
   "metadata": {},
   "source": [
    "## Write to your Databricks Delta Lake\n",
    "If you need to write your data into a Delta Lake instance you can also leverage your Databricks Delta Lake connector for the following actions:\n",
    "\n",
    "- Write the data into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0d8cfc-4146-4332-b205-7d080c374ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 2024-06-11 22:14:05,486 Some parameters are only considered for CSV files: \"sep\", \"has_header\" and \"columns\".\n"
     ]
    }
   ],
   "source": [
    "# Write the data to a new table called cardio_test in the set schema\n",
    "# If exists allow you to decide whether you want to append, replace or fail in case a table with the same name already exists in the schema.\n",
    "connector.write_table(data=query_output,\n",
    "                      staging_path='s3://ydata-dev/regular/internet_sales/test.csv',\n",
    "                      table='cardio_new',\n",
    "                      warehouse='Starter Warehouse',\n",
    "                      if_exists='fail')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
